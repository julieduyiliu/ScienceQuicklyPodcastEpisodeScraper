{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling using LDA\n",
    "Instead of mannully catagorize these episode, we can also use LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Authors</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infinite Tiling Presents a Modern Mathematical...</td>\n",
       "      <td>OCTOBER 4, 2024</td>\n",
       "      <td>MATH</td>\n",
       "      <td>Today’s mathematicians grapple with higher-ord...</td>\n",
       "      <td>KYNE SANTOS, RACHEL FELTMAN, FONDA MWANGI, MAD...</td>\n",
       "      <td>Infinite Tiling Presents a Modern Mathematical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Understanding Suzetrigine, a New Drug That Tre...</td>\n",
       "      <td>OCTOBER 2, 2024</td>\n",
       "      <td>MEDICINE</td>\n",
       "      <td>A new class of drugs treats pain at the periph...</td>\n",
       "      <td>RACHEL FELTMAN, MARLA BROADFOOT, FONDA MWANGI</td>\n",
       "      <td>Understanding Suzetrigine, a New Drug That Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An Effort to Fight against the Spread of Misin...</td>\n",
       "      <td>SEPTEMBER 30, 2024</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>Here’s how misinformation and distrust in scie...</td>\n",
       "      <td>RACHEL FELTMAN, FONDA MWANGI, ANAISSA RUIZ TEJADA</td>\n",
       "      <td>An Effort to Fight against the Spread of Misin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Math Part of Nature or an Invention of the ...</td>\n",
       "      <td>SEPTEMBER 27, 2024</td>\n",
       "      <td>MATH</td>\n",
       "      <td>Mathematics communicator and drag queen Kyne S...</td>\n",
       "      <td>KYNE SANTOS, RACHEL FELTMAN, FONDA MWANGI, MAD...</td>\n",
       "      <td>Is Math Part of Nature or an Invention of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People with PCOS Face Increased Eating Disorde...</td>\n",
       "      <td>SEPTEMBER 25, 2024</td>\n",
       "      <td>HEALTH CARE</td>\n",
       "      <td>A study reports higher prevalence of eating di...</td>\n",
       "      <td>RACHEL FELTMAN, FONDA MWANGI, JEFFERY DELVISCIO</td>\n",
       "      <td>People with PCOS Face Increased Eating Disorde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                Date  \\\n",
       "0  Infinite Tiling Presents a Modern Mathematical...     OCTOBER 4, 2024   \n",
       "1  Understanding Suzetrigine, a New Drug That Tre...     OCTOBER 2, 2024   \n",
       "2  An Effort to Fight against the Spread of Misin...  SEPTEMBER 30, 2024   \n",
       "3  Is Math Part of Nature or an Invention of the ...  SEPTEMBER 27, 2024   \n",
       "4  People with PCOS Face Increased Eating Disorde...  SEPTEMBER 25, 2024   \n",
       "\n",
       "      Category                                            Summary  \\\n",
       "0         MATH  Today’s mathematicians grapple with higher-ord...   \n",
       "1     MEDICINE  A new class of drugs treats pain at the periph...   \n",
       "2   TECHNOLOGY  Here’s how misinformation and distrust in scie...   \n",
       "3         MATH  Mathematics communicator and drag queen Kyne S...   \n",
       "4  HEALTH CARE  A study reports higher prevalence of eating di...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  KYNE SANTOS, RACHEL FELTMAN, FONDA MWANGI, MAD...   \n",
       "1      RACHEL FELTMAN, MARLA BROADFOOT, FONDA MWANGI   \n",
       "2  RACHEL FELTMAN, FONDA MWANGI, ANAISSA RUIZ TEJADA   \n",
       "3  KYNE SANTOS, RACHEL FELTMAN, FONDA MWANGI, MAD...   \n",
       "4    RACHEL FELTMAN, FONDA MWANGI, JEFFERY DELVISCIO   \n",
       "\n",
       "                                       combined_text  \n",
       "0  Infinite Tiling Presents a Modern Mathematical...  \n",
       "1  Understanding Suzetrigine, a New Drug That Tre...  \n",
       "2  An Effort to Fight against the Spread of Misin...  \n",
       "3  Is Math Part of Nature or an Invention of the ...  \n",
       "4  People with PCOS Face Increased Eating Disorde...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('podcast_episodes.csv')\n",
    "# combine the episode title and episode description into a single column\n",
    "df['combined_text'] = df['Title'] + ' ' + df['Summary']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found by LDA using TF-IDF:\n",
      "Topic 1: brief, around, one, technology, report, prize, kavli, including, news, world\n",
      "Topic 2: dog, human, year, intelligence, good, artificial, getting, get, could, health\n",
      "Topic 3: award, bird, change, climate, sponsored, cancer, problem, day, host, work\n",
      "Topic 4: human, year, ai, one, past, ice, help, may, space, moon\n",
      "Topic 5: nuclear, report, christopher, music, intagliata, space, lung, reservation, would, weapon\n",
      "Topic 6: face, immunity, dna, shot, math, talk, hope, large, likely, cleo\n",
      "Topic 7: election, talk, offer, turtle, language, get, dinosaur, key, sea, arctic\n",
      "Topic 8: covid, vaccine, pandemic, health, variant, today, editor, senior, american, josh\n",
      "Topic 9: bird, brain, sound, people, bat, help, love, get, secret, may\n",
      "Topic 10: could, life, might, mean, make, something, food, mind, year, wildlife\n",
      "\n",
      "Number of documents in each topic:\n",
      "Dominant_Topic\n",
      "0    71\n",
      "1    54\n",
      "2    54\n",
      "3    65\n",
      "4    52\n",
      "5    43\n",
      "6    56\n",
      "7    89\n",
      "8    57\n",
      "9    59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords and custom words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = {'new', 'research', 'scientists', 'researchers', 'study', 'find', 'found', 'show', 'shows', 'science', 'scientific', 'american', 'podcast', 'episode', 'quickly'}\n",
    "    stop_words.update(custom_stop_words)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the combined text data\n",
    "preprocessed_data = [preprocess_text(text) for text in df['combined_text']]\n",
    "\n",
    "# Create and fit the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "# Create and fit the LDA model\n",
    "n_topics = 10  # You can adjust this number\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_output = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Function to print top words for each topic\n",
    "def print_topics(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Print the topics\n",
    "print(\"Topics found by LDA using TF-IDF:\")\n",
    "print_topics(lda_model, vectorizer.get_feature_names_out(), 10)\n",
    "\n",
    "# Assign topics to each document\n",
    "df['Dominant_Topic'] = lda_output.argmax(axis=1)\n",
    "\n",
    "# Count the number of documents in each topic\n",
    "topic_counts = df['Dominant_Topic'].value_counts().sort_index()\n",
    "print(\"\\nNumber of documents in each topic:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Category                            Dominant_Topic\n",
      "0               MATH     Nuclear Weapons and Science Reporting\n",
      "1           MEDICINE   Environmental Conservation and Politics\n",
      "2         TECHNOLOGY             Wildlife and Human Experience\n",
      "3               MATH     Immunology, Genetics, and Mathematics\n",
      "4        HEALTH CARE      COVID-19 Pandemic and Health Updates\n",
      "..               ...                                       ...\n",
      "595  SPACE & PHYSICS             Wildlife and Human Experience\n",
      "596          BIOLOGY  Artificial Intelligence and Human Health\n",
      "597      ENVIRONMENT     Animal Communication and Neuroscience\n",
      "598     THE SCIENCES     Immunology, Genetics, and Mathematics\n",
      "599          BIOLOGY                  Space Exploration and AI\n",
      "\n",
      "[600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# give each topic a name\n",
    "topic_names = {\n",
    "    0: \"Science Awards and Technology News\",\n",
    "    1: \"Artificial Intelligence and Human Health\",\n",
    "    2: \"Climate Change and Environmental Challenges\",\n",
    "    3: \"Space Exploration and AI\",\n",
    "    4: \"Nuclear Weapons and Science Reporting\",\n",
    "    5: \"Immunology, Genetics, and Mathematics\",\n",
    "    6: \"Environmental Conservation and Politics\",\n",
    "    7: \"COVID-19 Pandemic and Health Updates\",\n",
    "    8: \"Animal Communication and Neuroscience\",\n",
    "    9: \"Wildlife and Human Experience\"\n",
    "}\n",
    "\n",
    "# add the topic names to the dataframe\n",
    "df['Dominant_Topic'] = df['Dominant_Topic'].map(topic_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
